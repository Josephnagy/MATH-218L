{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Make a copy of this notebook (File menu -> Make a Copy...)\n",
    "\n",
    "##### Throughout this week's homework, use your LUdet(A) function that you wrote in lab.\n",
    "\n",
    "### Homework Question 1\n",
    "\n",
    "1. Implement a function called *Cramers(A,v)* that solves the matrix equation $Ax=v$ using Cramer's Rule. If you need a reminder of Cramer's Rule, [see here](https://en.wikipedia.org/wiki/Cramer%27s_rule).<br><br>\n",
    "\n",
    "1. Consider the matrix equation $Ax=v$ where <br><br>\n",
    "   \n",
    "   $$A=\\begin{bmatrix}-4 & 1 & -9 &  4 &  4 &  1 &  9 & -6\\\\\n",
    "-1 & -3 & -8 & -2 & -4 &  4 &  2 &  7\\\\\n",
    " 3 &  6 & -8 &  1 &  0 &  1 &  2 &  5\\\\\n",
    "-10 & -4 & -1 & -4 &  2 & -8 &  3 &  2\\\\\n",
    "-6 &  7 & -4 & -6 &  2 &  1 & -4 &  8\\\\\n",
    " 6 &  6 &  5 &  8 & -7 &  9 & -1 &  9\\\\\n",
    " 1 & -10 &  2 & -6 & -8 & -9 & -4 &  0\\\\\n",
    " 6 &  1 & -4 &  3 &  8 &  3 &  3 &  5\\end{bmatrix}\\mbox{ and }v=\\begin{bmatrix} 28 \\\\ 40 \\\\ 45 \\\\ -18 \\\\ 26 \\\\ 114 \\\\ -125 \\\\ 102\\end{bmatrix}$$<br><br> \n",
    " \n",
    "      Solve these using both your Cramer's Rule function and previous solution techniques. Run timing comparisons and comment on your results. For your convenience, here is code for those arrays:\n",
    "      \n",
    "```python\n",
    "A = np.array([[4 , 1 , -9 ,  4 ,  4 ,  1 ,  9 , -6],[1 , -3 , -8 , -2 , -4 ,  4 ,  2 ,  7],[3 ,  6 , -8 ,  1 ,  0 ,  1 ,  2 ,  5],[10 , -4 , -1 , -4 ,  2 , -8 ,  3 ,  2],[6 ,  7 , -4 , -6 ,  2 ,  1 , -4 ,  8],[6 ,  6 ,  5 ,  8 , -7 ,  9 , -1 ,  9],[1 , -10 ,  2 , -6 , -8 , -9 , -4 ,  0],[6 ,  1 , -4 ,  3 ,  8 ,  3 ,  3 ,  5]])\n",
    "\n",
    "v=np.array([28,40,45,-18,26,114,-125,102])\n",
    "```\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.1055912665912414e-15, 1.0000000000000002, 1.999999999999991, 2.999999999999994, 3.9999999999999982, 5.0, 6.0, 7.000000000000002]\n",
      "286 µs ± 3.81 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "2.68 ms ± 69 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "191 ms ± 2.71 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rref import rref \n",
    "\n",
    "def rowaddmult(A,i,j,c):\n",
    "    A[j] = A[j] + (c * A[i])\n",
    "    return A\n",
    "\n",
    "def swaprows(A,i,j):\n",
    "    A[[i,j]] = A[[j,i]]\n",
    "    return A\n",
    "\n",
    "def rowredpivot(a):\n",
    "    copy = a.copy().astype(\"float\")\n",
    "    rows,cols = copy.shape\n",
    "    for i in range(rows):\n",
    "        slice = copy[i:,i] #look at the current column from the current row\n",
    "        slice = np.absolute(slice) #Take the absolute value of all the entries\n",
    "        swaprows(copy,i,np.argmax(slice)+i)\n",
    "        for j in range(i+1,rows): #row reduce by multiply by scalars and adding\n",
    "            rowaddmult(copy,i,j,(-1*(copy[j,i]/copy[i,i])))\n",
    "    return copy\n",
    "\n",
    "def backsub(U,v):\n",
    "    rows,cols = U.shape\n",
    "    res = np.zeros(cols)\n",
    "    for row in range(rows-1,-1,-1):\n",
    "        res[row] = (v[row] - res[row:]@U[row,row:])/U[row,row]\n",
    "    return res\n",
    "\n",
    "def rowred(a):\n",
    "    copy = a.copy().astype(\"float\")\n",
    "    rows,cols = copy.shape\n",
    "    for i in range(rows):#tracks the current row\n",
    "        for j in range(i+1,rows):#performs operations on the rows below the current one\n",
    "            rowaddmult(copy,i,j,(-1*(copy[j,i]/copy[i,i])))\n",
    "    return copy\n",
    "\n",
    "def rowredinv(a):\n",
    "    copy = a.copy().astype(\"float\")\n",
    "    rows,cols = copy.shape\n",
    "    identity = np.identity(rows)\n",
    "    a_augment_i = np.zeros((rows,cols*2))\n",
    "    a_augment_i[:,:cols] = copy\n",
    "    a_augment_i[:,cols:] = identity\n",
    "    reduced = rref(a_augment_i,10**-10)\n",
    "    return reduced[:,cols:]\n",
    "\n",
    "def det(A):\n",
    "    rows,cols = A.shape\n",
    "    deter = 0\n",
    "    if rows != cols:\n",
    "        raise ValueError(\"The matrix is not square\")\n",
    "    elif rows == 2:\n",
    "        return det2(A)\n",
    "    elif rows == 3:\n",
    "        return det3(A)\n",
    "    else:\n",
    "        for i in range(rows):\n",
    "            deter += ((-1)**(i+2))*A[0][i]*det(minor(A,1,i+1))\n",
    "        return deter\n",
    "\n",
    "def minor(A,i,j):\n",
    "    rows,cols = A.shape\n",
    "    res = np.zeros((rows-1,cols-1))\n",
    "    \n",
    "    res[:i-1,:j-1] = A[:i-1,:j-1]\n",
    "    res[:i-1,j-1:] = A[:i-1,j:]\n",
    "    res[i-1:,:j-1] = A[i:,:j-1]\n",
    "    res[i-1:,j-1:] = A[i:,j:]\n",
    "    \n",
    "    return res\n",
    "\n",
    "def det3(A):\n",
    "    accum = 0\n",
    "    for i in range(3):\n",
    "        accum += ((-1)**(2+i)) * A[0][i]*det2(minor(A,1,i+1))\n",
    "    return accum\n",
    "def det2(A):\n",
    "    row, col = A.shape\n",
    "    if row != 2 or col !=2:\n",
    "        raise ValueError(\"The matrix is not a 2x2\")\n",
    "    return A[0][0]*A[1][1] - A[0][1]*A[1][0]\n",
    "\n",
    "def LU(A):\n",
    "    U = A.copy().astype(\"float\")\n",
    "    rows,cols = U.shape\n",
    "    L = np.zeros_like(U)\n",
    "    P = np.eye(rows)\n",
    "    swapCounter = 0\n",
    "    for i in range(rows):\n",
    "        slice = U[i:,i] #look at the current column from the current row\n",
    "        slice = np.absolute(slice) #Take the absolute value of all the entries\n",
    "        index = np.argmax(slice)+i\n",
    "        if i != index: #if the index of the current row isn't the index of the of the max entry, swap the two rows\n",
    "            swaprows(U,i,index)\n",
    "            swaprows(L,i,index)\n",
    "            swaprows(P,i,index)\n",
    "            swapCounter += 1\n",
    "        for j in range(i+1,rows): #row reduce by multiply by scalars and adding\n",
    "            holder = U[j,i]/U[i,i]\n",
    "            L[j,i] = holder\n",
    "            rowaddmult(U,i,j,(-1*holder))\n",
    "    np.fill_diagonal(L,1.)         \n",
    "    return (L, U, P, swapCounter)\n",
    "\n",
    "def LUdet(A):\n",
    "    L,U,P,swapCounter = LU(A)\n",
    "    return U.diagonal().prod()/((-1)**swapCounter)\n",
    "\n",
    "\n",
    "def Cramers(A,v):\n",
    "    # calculate det(A) using LUdet(A)\n",
    "    D = LUdet(A)\n",
    "    x_values = []  #create array to store x's \n",
    "    \n",
    "    for i in range(A.shape[0]):\n",
    "        B = A.copy() \n",
    "        B[:,i] = v \n",
    "        x_values.append(LUdet(B)/D)\n",
    "        \n",
    "        \n",
    "    return x_values\n",
    "\n",
    "#test array \n",
    "A = np.array([[-4 , 1 , -9 , 4 , 4 , 1 , 9 , -6],\n",
    "[-1 , -3 , -8 , -2 , -4 , 4 , 2 , 7],\n",
    "[3 , 6 , -8 , 1 , 0 , 1 , 2 , 5],\n",
    "[-10 , -4 , -1 , -4 , 2 , -8 , 3 , 2],\n",
    "[-6 , 7 , -4 , -6 , 2 , 1 , -4 , 8],\n",
    "[6 , 6 , 5 , 8 , -7 , 9 , -1 , 9],\n",
    "[1 , -10 , 2 , -6 , -8 , -9 , -4 , 0],\n",
    "[6 , 1 , -4 , 3 , 8 , 3 , 3 , 5]]).astype(float)\n",
    "\n",
    "v=np.array([28,40,45,-18,26,114,-125,102])\n",
    "\n",
    "print(Cramers(A,v))\n",
    "%timeit LUdet(A)\n",
    "%timeit Cramers(A,v)\n",
    "%timeit det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity analysis: \n",
    "We found that the fastest function was LUdet() which ran in 316 µs. The second fastest function was Cramers() which ran in 2.69 ms. The slowest function was det() as expected which ran in 194 ms. This was expected as the body of the Cramers() calls LUdet(), so naturally Cramers() would have to be slower than LUdet(). Likewise, we expected det() to be the slowest function as it requires the most computation with its recursive algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework Question 2\n",
    "\n",
    "The inverse $A^{-1}$ of a matrix can be found using the *cofactor matrix*. If $A$ is a matrix<br><br> $$A=\\begin{bmatrix}a_{1,1} & \\cdots & a_{1,n} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & \\cdots & a_{n,n}\\end{bmatrix}$$<br><br> then its *cofactor matrix* is defined to be $$adj(A)=\\begin{bmatrix}C_{1,1} & \\cdots & C_{1,n} \\\\ \\vdots & \\ddots & \\vdots \\\\ C_{n,1} & \\cdots & C_{n,n}\\end{bmatrix}^T,$$<br><br> where $C_{i,j}$ is the $(i,j)$ cofactor of $a_{i,j}$ (note the transpose sign!).<br><br>\n",
    "\n",
    "1. Find by hand the cofactor matrix of <br><br>$$A=\\begin{bmatrix} -2 & 11 & 48 \\\\-2 & 11 & 49 \\\\3 & -16 & -70\n",
    "\\end{bmatrix}.$$<br><br>\n",
    "1. Find the determinant of $A$, and show that $A\\cdot adj(A)=det(A)\\cdot I_3$ in this case (where $I_3$ is the $3\\times 3$ identity matrix).<br><br>\n",
    "1. This property is true in general. Use this to write a function called `detInv(A)` that computes the inverse of $A$ using determinants. Try to avoid computing cofactors twice!<br><br>\n",
    "1. Use your function to compute the inverse of the matrix from the first part of this question above.<br><br>\n",
    "1. Is this an efficient way to compute inverses? Experimentally show that once again, using row reduction is far better. (See [here](https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html) if you need a reminder as to how to find inverses using row reduction. Feel free to use the `rref(A,tol)` function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: \n",
    "This is NOT an efficient way to compute inverses. This is because first you need to compute the cofactor matrix which involves a double-nested for-loop which is computation intensive. You also have to calculate the determinant and perform row operations to calculate the PA=LU decomposition. Meanwhile, computing the inverse via row reduction only requires row operations and does not require further computation such as calculating the cofactor matrix or determinant. In short, this method of computing the inverse involves much more computation than is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cofactor matrix of: \n",
      "\n",
      "[[ -2  11  48]\n",
      " [ -2  11  49]\n",
      " [  3 -16 -70]]\n",
      "\n",
      "is:\n",
      "\n",
      "[[14  7 -1]\n",
      " [ 2 -4  1]\n",
      " [11  2  0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 1: We found that our cofactor matrix was the same when computed by hand and \n",
    "#via our cofactor algorithm. The answer we got for both is: \n",
    "A = np.array([[-2,11,48],[-2,11,49],[3,-16,-70]])\n",
    "\n",
    "print(\"Cofactor matrix of: \",end=\"\\n\\n\")\n",
    "print(C,end=\"\\n\\n\")\n",
    "print(\"is:\",end=\"\\n\\n\")\n",
    "print(cofactor(C),end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant of A is: \n",
      "\n",
      "1.0000000000000018\n",
      "\n",
      "The property holds because both matrices are equal: \n",
      "\n",
      "[[ True  True  True  True]\n",
      " [ True  True  True  True]\n",
      " [ True  True  True  True]\n",
      " [ True  True  True  True]]\n",
      "\n",
      "The inverse of A calculated via detInv() is: \n",
      "\n",
      "[[14.  2. 11.]\n",
      " [ 7. -4.  2.]\n",
      " [-1.  1.  0.]]\n",
      "\n",
      "The inverse of A calculated via detInv(A) equal to np.linalg.inv(A): \n",
      "\n",
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n",
      "\n",
      "Time to compute the inverse of A via detInv():\n",
      "\n",
      "1.56 ms ± 20.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "\n",
      "\n",
      "Time to compute the inverse of A via rowredinv():\n",
      "\n",
      "107 µs ± 1.12 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def cofactor(A):\n",
    "    C = np.zeros_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            m = minor(A,i+1,j+1)\n",
    "            cofactor = ((-1)**(i+j)) * LUdet(m)\n",
    "            C[i,j] = cofactor \n",
    "    return C \n",
    "\n",
    "def detInv(A):\n",
    "    C = cofactor(A)\n",
    "    return C.T/LUdet(A)\n",
    "    \n",
    "\n",
    "\n",
    "# Question 2 \n",
    "print(\"Determinant of A is: \", end=\"\\n\\n\")\n",
    "print(LUdet(A),end=\"\\n\\n\")\n",
    "print(\"The property holds because both matrices are equal: \",end=\"\\n\\n\")\n",
    "print(np.isclose(B@cofactor(B).T,LUdet(B)*np.identity(4)), end=\"\\n\\n\")\n",
    "\n",
    "# Question 3: \n",
    "print(\"The inverse of A calculated via detInv() is: \", end=\"\\n\\n\")\n",
    "print(detInv(A),end=\"\\n\\n\")\n",
    "\n",
    "# Question 4\n",
    "print(\"The inverse of A calculated via detInv(A) equal to np.linalg.inv(A): \", end=\"\\n\\n\")\n",
    "print(np.isclose(np.linalg.inv(A),detInv(A)),end=\"\\n\\n\")\n",
    "\n",
    "#Question 5\n",
    "print(\"Time to compute the inverse of A via detInv():\",end=\"\\n\\n\")\n",
    "%timeit detInv(B)\n",
    "print(\"\\n\")\n",
    "print(\"Time to compute the inverse of A via rowredinv():\",end=\"\\n\\n\")\n",
    "%timeit rowredinv(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework Question 3\n",
    "\n",
    "It is often said that determinants are one of the least useful ways to do numerical computation with matrices. Do some research online and read a paper or two, then write a paragraph summarizing why this is the case. Include references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our research, determinants were extremely important in the early days of mathematics because they were used to explain mathematical theory and gain a greater understanding of linear algebra as a whole. However, it was soon realized that calculating determinants is extremely computation heavy. As most computer science students are aware, the speed of an algorithm is measured by finding how the time taken by the computer grows as the size of its input data set grows. In other words, the speed of an algorithm is measured by it’s asymptotic growth with respect to the size of its input. Computing determinants has a runtime of O(n!), which is one of the slowest algorithms possible. For example, if one wanted to calculate the determinant of a 10 x 10 matrix, it would have over 10! (3,628,800) operations to compute, making it extremely slow and inefficient. There has been some research in trying to make calculating determinants faster, such as via “dynamic determinant computation” which can do some determinant calculations in cubic runtime, but this is a relatively new breakthrough. Likewise, a cubic runtime is also extremely slow and there are numerous other methods that could be used without taking determinants that are much faster. \n",
    "\n",
    "References: \n",
    "1) https://en.wikibooks.org/wiki/Linear_Algebra/Topic:_Speed_of_Calculating_Determinants\n",
    "2) https://arxiv.org/pdf/1206.7067.pdf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
