{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Make a copy of this notebook (File menu -> Make a Copy...)\n",
    "\n",
    "### Homework Question 1\n",
    "\n",
    "This question will explore the difference between classical Gram-Schmidt (GS) and Modified Gram-Schmidt. We use two matrices, one random matrix generated by you, and a second matrix designed to highlight the difference.\n",
    "\n",
    "1. In lab, you wrote code for MGS. Write a similar function for GS. Show a test on a small matrix. You should get the same result as your MGS function.<br><br>\n",
    "1. The *$n\\times n$ Hilbert matrix* has entry $\\frac{1}{i+j-1}$ in the $(i,j)$ position (where $i$ and $j$ are numbered from 1 to $n$). Write a function `hilbert(n)` that returns this matrix. You can use loops if you like. Careful to translate from NumPy (0 to $n-1$) numbering into classical numbering. Show that you get the correct matrix for $n=3$.<br><br>\n",
    "1. GS and MGS produce orthogonal matrices. If $Q$ is an orthogonal matrix, what is $Q^TQ$?\n",
    "> We will test how good our functions are by testing how far away from the desired result our actual result is. To do this, we will take our result, $Q$, subtract $Q^TQ$ from the matrix we 'should' get (the answer to Part 3 of this question), and find the *matrix norm* of the result. The matrix norm is simply the square root of the sum of the entries of the matrix, and can be computed using `np.linalg.norm(A)`. Call this number the *error*.\n",
    "1. Generate a random $200\\times 200$ matrix using `np.random.rand(200,200)`. Compute the errors we get using each of GS and MGS.<br><br>\n",
    "1. Consider the matrix `0.00001*np.eye(n)+hilbert(n)`. Compute the errors for this matrix using each of GS and MGS.<br><br>\n",
    "1. Comment briefly on the results in the previous two parts of this question.\n",
    "\n",
    "**Note:** While a complete error analysis is beyond the scope of this class, see [here](https://www.math.uci.edu/~ttrogdon/105A/html/Lecture23.html) if you're interested in why MGS is so much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework Question 2\n",
    "\n",
    "Choose at least one of the following to extend your handwriting recognition work. Doing both well will get extra credit. If you do both, you will have created a handwritten digit recognition system that is as good as can be given only the tools we have.\n",
    "\n",
    "#### Extending your code to compute binary classifiers for all ten digits\n",
    "1. Compute binary classifiers for each of the digits between 0 and 9.<br><br>\n",
    "1. Compute error rates for each digit, both on the training set and the test set.<br><br>\n",
    "1. For each image in the test set, use all ten classifiers to see how many handwritten digits your classifiers give unique answers to. That is, if your classifiers determine that a particular image may be a 1 or a 2, then you cannot classify that particular image. How many images were not recognized as any digit at all?<br><br>\n",
    "1. Compute your overall success rate on the test images. That is, compute how many images were correctly and uniquely classified. Also compute the rate of *false positives* (that is, images that were identified as digits, but whose label was wrong), and *false negatives* (images whose digit couldn't be assigned). This is how good your handwriting recognition is!\n",
    "    \n",
    "#### Optimizing your classifier(s)\n",
    "1. We decided that a negative result from the model mean 'this image is not 0', and non-negative results means 'this image is 0'. Write code that searches for a better threshold than 0. That is, find the threshold that gives you the lowest error rate on the test set.<br><br>\n",
    "1. What error rate does your optimized threshold give on the test set?<br><br>\n",
    "1. Why can't you use the test set to optimize the threshold?\n",
    "    \n",
    "##### Note: if you do choose to do both, the thresholds may well not be the same for each digit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error rate for 0\n",
      "0.015566666666666666\n",
      "Testing error rate for 0\n",
      "0.0157\n",
      "Training error rate for 1\n",
      "0.019016666666666668\n",
      "Testing error rate for 1\n",
      "0.0166\n",
      "Training error rate for 2\n",
      "0.03766666666666667\n",
      "Testing error rate for 2\n",
      "0.0416\n",
      "Training error rate for 3\n",
      "0.04223333333333333\n",
      "Testing error rate for 3\n",
      "0.0398\n",
      "Training error rate for 4\n",
      "0.03378333333333333\n",
      "Testing error rate for 4\n",
      "0.0335\n",
      "Training error rate for 5\n",
      "0.05556666666666667\n",
      "Testing error rate for 5\n",
      "0.0536\n",
      "Training error rate for 6\n",
      "0.021666666666666667\n",
      "Testing error rate for 6\n",
      "0.026\n",
      "Training error rate for 7\n",
      "0.03398333333333333\n",
      "Testing error rate for 7\n",
      "0.0354\n",
      "Training error rate for 8\n",
      "0.05135\n",
      "Testing error rate for 8\n",
      "0.051\n",
      "Training error rate for 9\n",
      "0.05563333333333333\n",
      "Testing error rate for 9\n",
      "0.052\n"
     ]
    }
   ],
   "source": [
    "from MNISTHandwriting import readimgs\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as LA\n",
    "tol = 10**-10\n",
    "\n",
    "#setting up images and matrices for all training-set classifiers\n",
    "imagesTrain =  readimgs('./data/train-images-idx3-ubyte')[0].astype('float')\n",
    "labelsTrain = readimgs('./data/train-labels-idx1-ubyte')[0].astype('float')\n",
    "\n",
    "A = np.reshape(imagesTrain,(60000,784))\n",
    "sumOfCols = A.sum(axis=0)\n",
    "index = np.nonzero(sumOfCols)[0]\n",
    "\n",
    "B = np.ones((60000,718))\n",
    "B[:,:-1] = A[:,index]\n",
    "\n",
    "#setting up images and matrices for testing-set classifiers\n",
    "imagesTest = readimgs('./data/t10k-images-idx3-ubyte')[0].astype('float')\n",
    "labelsTest = readimgs('./data/t10k-labels-idx1-ubyte')[0].astype('float')\n",
    "\n",
    "C = np.reshape(imagesTest,(10000,784))\n",
    "C = C[:,index]\n",
    "D = np.ones((10000,718))\n",
    "D[:,:-1] = C\n",
    "\n",
    "def makeClassifier(num):\n",
    "    #making an array with -1 and 1 depending on label for training set\n",
    "    classifiedLabelTrain = np.ones(60000)\n",
    "    classifiedLabelTrain[labelsTrain != num] = -1\n",
    "    \n",
    "    #making an array with -1 and 1 depending on label for testing set\n",
    "    classifiedLabelTest = np.ones(10000)\n",
    "    classifiedLabelTest[labelsTest != num] = -1\n",
    "    \n",
    "    #calculating the coefficients of our predictor\n",
    "    ans = LA.lstsq(B,classifiedLabelTrain,rcond=tol)[0]\n",
    "    \n",
    "    #calculating A@xhat for training and testing sets\n",
    "    evalTrain = B@ans\n",
    "    evalTest = D@ans\n",
    "    \n",
    "    #creating array of -1 and 1 depending on the training set results\n",
    "    modelresultsTrain = np.ones(60000)\n",
    "    modelresultsTrain[evalTrain <= 0] = -1\n",
    "    \n",
    "    #creating array of -1 and 1 depending on the testing set results\n",
    "    modelresultsTest = np.ones(10000)\n",
    "    modelresultsTest[evalTest <= 0] = -1\n",
    "    return modelresultsTrain, classifiedLabelTrain, modelresultsTest, classifiedLabelTest\n",
    "\n",
    "\n",
    "def errorRates(learnedLabels,correctLabels,size):\n",
    "    #compare is a 1x10000 boolean array, where True if our model correctly identified the digit\n",
    "    compare = (correctLabels == learnedLabels)\n",
    "    return (len(compare[compare == False])/size), compare\n",
    "\n",
    "#make a 10x10000 array of zeros; insert modelresTest into corresponding rows of resUnique, which will\n",
    "#be used to compute the number of uniquely identified images\n",
    "dummy = np.zeros((10000))\n",
    "resUnique = np.tile(dummy,10).reshape((10,10000))\n",
    "\n",
    "#make a 10x10000 array of falses; insert compare into corresponding rows of resCorrect, which will\n",
    "#be used to compute the number of correctly identified images\n",
    "resCorrect = np.ones((10,10000),dtype=bool)\n",
    "\n",
    "#make a 10x10000 array of zeros; insert modelresTest into corresponding rows of modelResults, which will\n",
    "#be used to compute the error\n",
    "modelResults = np.tile(dummy,10).reshape((10,10000))\n",
    "\n",
    "#make a 10x10000 array of zeros; insert modelresTest into corresponding rows of classifiedResults, which will\n",
    "#be used to compute the error\n",
    "classifiedResults = np.tile(dummy,10).reshape((10,10000))\n",
    "\n",
    "for i in range(10):\n",
    "    modelresTrain,classLabelTrain,modelresTest,classLabelTest = makeClassifier(i)\n",
    "    modelResults[i] = modelresTest\n",
    "    classifiedResults[i] = classLabelTest\n",
    "    resUnique[i] = modelresTest\n",
    "    print(\"Training error rate for \"+ str(i))\n",
    "    print(errorRates(modelresTrain,classLabelTrain,60000)[0])\n",
    "    print(\"Testing error rate for \" + str(i))\n",
    "    errorTests, errorArray = errorRates(modelresTest,classLabelTest,10000)\n",
    "    print(errorTests)\n",
    "    resCorrect[i] = errorArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of recognized images: \n",
      "7050\n",
      "The number of unrecognized images: \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "sumOfCols = resUnique.sum(axis=0)\n",
    "\n",
    "#classifier uniquely idenitifies image: only one classifier returns 1 for that image and all others\n",
    "#return -1, producing a sum of -8\n",
    "worksCorrectly = sumOfCols[sumOfCols == -8]\n",
    "print(\"The number of recognized images: \")\n",
    "print(len(worksCorrectly))\n",
    "\n",
    "#classifiers cannot recognize image at all: all classifiers return -1, producing a sum of -9\n",
    "unrecognized = sumOfCols[sumOfCols == -9]\n",
    "print(\"The number of unrecognized images: \")\n",
    "print(len(unrecognized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our overall success rate: \n",
      "0.6711\n",
      "Total number of false positives: \n",
      "520\n",
      "Total number of false negatives: \n",
      "3132\n"
     ]
    }
   ],
   "source": [
    "#boolean array of correctly identified images\n",
    "resCorrectCollapsed = resCorrect.all(axis=0)\n",
    "\n",
    "#boolean array of uniquely identified images\n",
    "uniques = np.zeros(10000,dtype=bool)\n",
    "uniques[sumOfCols == -8] = True\n",
    "\n",
    "#and the arrays of correctly identified and uniquely identified images\n",
    "#count of Trues in resulting array is number of correctly AND uniquely identified images\n",
    "uniqueAndCorrect = np.stack((resCorrectCollapsed,uniques)).all(axis=0)\n",
    "\n",
    "print(\"This is our overall success rate: \")\n",
    "print(len(uniqueAndCorrect[uniqueAndCorrect==True])/10000)\n",
    "\n",
    "#calculating false positives\n",
    "totalfp = 0\n",
    "totalfn = 0\n",
    "\n",
    "for i in range(10):\n",
    "    #default boolean array of falses\n",
    "    fpModel = np.zeros(10000,dtype=bool)\n",
    "    fpClass = np.zeros(10000,dtype=bool)\n",
    "    fnModel = np.zeros(10000,dtype=bool)\n",
    "    fnClass = np.zeros(10000,dtype=bool)\n",
    "    \n",
    "    #check where the model returns 1 and where the classified label returns -1\n",
    "    fpModel[modelResults[i] == 1] = True\n",
    "    fpClass[classifiedResults[i] == -1] = True\n",
    "    \n",
    "    #check where the model returns -1 and where the classified label returns 1\n",
    "    fnModel[modelResults[i] == -1] = True\n",
    "    fnClass[classifiedResults[i] == 1] = True\n",
    "    \n",
    "    #where fpModel and fpClass are both True, we have a false positive\n",
    "    intersectionfp = np.stack((fpModel,fpClass)).all(axis=0)\n",
    "    totalfp += len(intersectionfp[intersectionfp == True])\n",
    "    \n",
    "    #where fnModel and fnClass are both True, we have a false negative\n",
    "    intersectionfn = np.stack((fnModel,fnClass)).all(axis=0)\n",
    "    totalfn += len(intersectionfn[intersectionfn == True])\n",
    "    \n",
    "print(\"Total number of false positives: \")\n",
    "print(totalfp)\n",
    "\n",
    "print(\"Total number of false negatives: \")\n",
    "print(totalfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the threshold with the lowest error rate for 0: \n",
      "-0.08099999999999918\n",
      "This is our error rate: \n",
      "1.3599999999999999%\n",
      "\n",
      "This is the threshold with the lowest error rate for 1: \n",
      "-0.010999999999999122\n",
      "This is our error rate: \n",
      "1.6500000000000001%\n",
      "\n",
      "This is the threshold with the lowest error rate for 2: \n",
      "-0.24699999999999933\n",
      "This is our error rate: \n",
      "3.1399999999999997%\n",
      "\n",
      "This is the threshold with the lowest error rate for 3: \n",
      "-0.14099999999999924\n",
      "This is our error rate: \n",
      "3.4099999999999997%\n",
      "\n",
      "This is the threshold with the lowest error rate for 4: \n",
      "-0.16499999999999926\n",
      "This is our error rate: \n",
      "2.8000000000000003%\n",
      "\n",
      "This is the threshold with the lowest error rate for 5: \n",
      "-0.3339999999999994\n",
      "This is our error rate: \n",
      "3.7800000000000002%\n",
      "\n",
      "This is the threshold with the lowest error rate for 6: \n",
      "-0.0949999999999992\n",
      "This is our error rate: \n",
      "2.33%\n",
      "\n",
      "This is the threshold with the lowest error rate for 7: \n",
      "-0.1009999999999992\n",
      "This is our error rate: \n",
      "3.18%\n",
      "\n",
      "This is the threshold with the lowest error rate for 8: \n",
      "-0.2219999999999993\n",
      "This is our error rate: \n",
      "4.2700000000000005%\n",
      "\n",
      "This is the threshold with the lowest error rate for 9: \n",
      "-0.2029999999999993\n",
      "This is our error rate: \n",
      "4.31%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def configureMatrices(digit):\n",
    "    #configuring -1 and 1 depending on training labels\n",
    "    classifiedLabelTrain = np.ones(60000)\n",
    "    classifiedLabelTrain[labelsTrain != digit] = -1\n",
    "    \n",
    "    #solving for xhat and D@xhat based on training set\n",
    "    ans = LA.lstsq(B,classifiedLabelTrain,rcond=tol)[0]\n",
    "    evalTrain = D@ans\n",
    "    \n",
    "    #initializing array of -1 and 1 depending on testing labels\n",
    "    classifiedLabelTest= np.ones(10000)\n",
    "    classifiedLabelTest[labelsTest != digit] =-1\n",
    "    \n",
    "    #initializing boolean arrays for false positives and negatives\n",
    "    fpClass = np.zeros(10000,dtype=bool)\n",
    "    fpClass[classifiedLabelTest == -1] = True\n",
    "    fnClass = np.zeros(10000,dtype=bool)\n",
    "    fnClass[classifiedLabelTest == 1] = True \n",
    "    return evalTrain,fpClass,fnClass\n",
    "\n",
    "def classifyGuess(guess,evalTest):\n",
    "    #assigning -1 and 1 depending on our threshold guess\n",
    "    modelresultsTest = np.ones(10000)\n",
    "    modelresultsTest[evalTest <= guess] = -1\n",
    "    return modelresultsTest\n",
    "\n",
    "#search algorithm that finds and saves the optimal threshold\n",
    "def threshold(begin, end, digit):\n",
    "    curr = begin\n",
    "    leastError = curr\n",
    "    totalFalse = 10000 \n",
    "    \n",
    "    evalTest,fpClass,fnClass = configureMatrices(digit)\n",
    "    while(curr <= end):\n",
    "        falseCounter = 0\n",
    "        \n",
    "        #default boolean array of false\n",
    "        fpModel = np.zeros(10000,dtype=bool)\n",
    "        fnModel = np.zeros(10000,dtype=bool)\n",
    "        \n",
    "        modelresultsTest = classifyGuess(curr,evalTest)\n",
    "        \n",
    "        #check where the model returns 1 and where the classified label returns -1\n",
    "        fpModel[modelresultsTest == 1] = True\n",
    "    \n",
    "        #check where the model returns -1 and where the classified label returns 1\n",
    "        fnModel[modelresultsTest == -1] = True\n",
    "    \n",
    "        #where fpModel and fpClass are both True, we have a false positive\n",
    "        intersectionfp = np.stack((fpModel,fpClass)).all(axis=0)\n",
    "        falseCounter += len(intersectionfp[intersectionfp == True])\n",
    "    \n",
    "        #where fnModel and fnClass are both True, we have a false negative\n",
    "        intersectionfn = np.stack((fnModel,fnClass)).all(axis=0)\n",
    "        falseCounter += len(intersectionfn[intersectionfn == True])\n",
    "\n",
    "        if(falseCounter < totalFalse):\n",
    "            leastError = curr\n",
    "            totalFalse = falseCounter\n",
    "        curr += 0.001\n",
    "    return leastError, totalFalse\n",
    "\n",
    "for i in range(10):\n",
    "    thresh,error = threshold(-1,1,i)\n",
    "    print(\"This is the threshold with the lowest error rate for \" + str(i) + \": \")\n",
    "    print(thresh)\n",
    "    print(\"This is our error rate: \")\n",
    "    print(str((error/10000) * 100) + \"%\", end=\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can't use the test set to optimize the threshold because if the test set is used, then we would be\n",
    "#overfitting the model, producing high bias for this test set. An accurate predictor would ideally be \n",
    "#consistent across multiple test sets, but if you optimize and overfit for this test set, it would be\n",
    "#useless for any data set in the real-world. Thus, the purpose of a training set is to find an ideal\n",
    "#model that works for multiple test sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
